{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89659207",
   "metadata": {},
   "source": [
    "![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625f718",
   "metadata": {},
   "source": [
    "# Text Mining: Models and Algorithms\n",
    "\n",
    "## Problem Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f10b564",
   "metadata": {},
   "source": [
    "### *1.⁠ ⁠Identify a (future) event that makes a lot of people come to Barcelona. Think about music festivals, local festivities etc. (2 points)*\n",
    "We have selected the Sónar festival. It is the 31st edition of the Barcelona International Festival of Advanced Music and Multimedia Art in 2024. This vibrant event takes place in Montjuic and attracts enthusiasts from all over the world to Barcelona to participate in its rich offer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd0077",
   "metadata": {},
   "source": [
    "### *2.⁠ ⁠Think of the time periods to scrape and what second city to scrape for these same timer periods. Explain your choices in written. (2 points)*\n",
    "\n",
    "The festival unfolds on June 13, 14, and 15, and we have opted to analyze the period preceding it, that is the corresponding days on June 6, 7, and 8, 2024. Ensuring an equivalent number of days and proximity to the event dates is crucial for a meaningful comparison of similar scenarios. We also include Valencia as the second city to control for due to its proximity and similarities to Barcelona. Both cities are located on Spain's eastern coastline along the Mediterranean Sea and share similar geographical and cultural situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7259e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, StaleElementReferenceException\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore SettingWithCopyWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
    "\n",
    "\n",
    "# Go get geckodriver from : https://github.com/mozilla/geckodriver/releases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073a3b0",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffx_preferences(dfolder, download=False):\n",
    "    '''\n",
    "    Sets the preferences of the firefox browser: download path.\n",
    "    '''\n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    # set download folder:\n",
    "    profile.set_preference(\"browser.download.dir\", dfolder)\n",
    "    profile.set_preference(\"browser.download.folderList\", 2)\n",
    "    profile.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\",\n",
    "                           \"application/msword,application/rtf, application/csv,text/csv,image/png ,image/jpeg, application/pdf, text/html,text/plain,application/octet-stream\")\n",
    "    \n",
    "    # profile.add_extension('/Users/luisignaciomenendezgarcia/Dropbox/CLASSES/class_bse_text_mining/class_scraping_bse/booking/booking/ublock_origin-1.55.0.xpi')\n",
    "\n",
    "\n",
    "    # this allows to download pdfs automatically\n",
    "    if download:\n",
    "        profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf,application/x-pdf\")\n",
    "        profile.set_preference(\"pdfjs.disabled\", True)\n",
    "\n",
    "    options = Options()\n",
    "    options.profile = profile\n",
    "    return options\n",
    "\n",
    "\n",
    "def start_up(link, dfolder, geko_path,donwload=True):\n",
    "    # geko_path='/Users/luisignaciomenendezgarcia/Dropbox/CLASSES/class_bse_text_mining/class_scraping_bse/booking/geckodriver'\n",
    "    # download_path='./downloads'\n",
    "    os.makedirs(dfolder, exist_ok=True)\n",
    "\n",
    "    options = ffx_preferences(dfolder,donwload)\n",
    "    service = Service(geko_path)\n",
    "    browser = webdriver.Firefox(service=service, options=options)\n",
    "    # Enter the website address here\n",
    "    browser.get(link)\n",
    "    time.sleep(5)  # Adjust sleep time as needed\n",
    "    return browser\n",
    "        \n",
    "def check_and_click(browser, xpath, type):\n",
    "    '''\n",
    "    Function that checks whether the object is clickable and, if so, clicks on\n",
    "    it. If not, waits one second and tries again.\n",
    "    '''\n",
    "    start_time = time.time()  # Record the start time\n",
    "    while True:\n",
    "        try:\n",
    "            element = browser.find_element(By.XPATH, xpath)\n",
    "            element.click()\n",
    "            return \"Clicked!\"  # Element found and clicked successfully\n",
    "        except NoSuchElementException:\n",
    "            pass  # Continue if element not found\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return False  # Other unexpected errors\n",
    "\n",
    "        time.sleep(1)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time >= 3:\n",
    "            # print(\"** The element was not found in the page. **\")\n",
    "            return None  # Element not found after 5 seconds\n",
    "        \n",
    "def check_obscures(browser, xpath, type):\n",
    "    '''\n",
    "    Function that checks whether the object is being \"obscured\" by any element so\n",
    "    that it is not clickable. Important: if True, the object is going to be clicked!\n",
    "    '''\n",
    "    try:\n",
    "        if type == \"xpath\":\n",
    "            browser.find_element('xpath', xpath).click()\n",
    "        elif type == \"id\":\n",
    "            browser.find_element('id', xpath).click()\n",
    "        elif type == \"css\":\n",
    "            browser.find_element('css selector', xpath).click()\n",
    "        elif type == \"class\":\n",
    "            browser.find_element('class name', xpath).click()\n",
    "        elif type == \"link\":\n",
    "            browser.find_element('link text', xpath).click()\n",
    "    except (ElementClickInterceptedException, StaleElementReferenceException) as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    except NoSuchElementException:\n",
    "        # Do nothing if NoSuchElementException occurs (suppress the error)\n",
    "        pass\n",
    "    return True\n",
    "\n",
    "def element_exists(browser, path):\n",
    "    try:\n",
    "        browser.find_element('xpath', path)\n",
    "        return True\n",
    "    except NoSuchElementException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6174d6",
   "metadata": {},
   "source": [
    "### Scraping Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca11b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrape:\n",
    "    '''\n",
    "    Class for web scraping accommodation information from Booking.com.\n",
    "\n",
    "    Attributes:\n",
    "    - browser: Selenium WebDriver instance for controlling the web browser.\n",
    "    - search_bar_xpath: XPath for the search bar on the Booking.com page.\n",
    "    - search_x_path: XPath for the search button on the page.\n",
    "    - date_button_css: CSS Selector for the date selection button.\n",
    "    - number_of_people_xpath: XPath for the button to select the number of people.\n",
    "    - search_button_xpath: XPath for the final search button.\n",
    "    - x_path_prev_date: XPath for the button to navigate to the previous date.\n",
    "    - x_path_next_date: XPath for the button to navigate to the next date.\n",
    "    - x_path_month: XPath for displaying the current month.\n",
    "    - people_path: XPath for the button to select the number of people.\n",
    "    - pages: Number of pages to scrape.\n",
    "    - headers: HTTP headers for making requests.\n",
    "    - data: Pandas DataFrame to store scraped information.\n",
    "    - place: Variable to store the destination place.\n",
    "\n",
    "    Methods:\n",
    "    - input_place(): Takes user input for the destination place and interacts with the search bar.\n",
    "    - input_dates(): Takes user input for the stay dates and interacts with the date selection.\n",
    "    - input_people(): Takes user input for the number of people and interacts with the selection.\n",
    "    - search(): Initiates the search for accommodations.\n",
    "    - get_pages(limit): Retrieves the total number of pages to scrape.\n",
    "    - scrape_info(): Scrapes general information about accommodations.\n",
    "    - scrape_description(url): Scrapes the description of an accommodation given its URL.\n",
    "    - get_descriptions(): Scrapes descriptions for all accommodations in parallel.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        print(\"Remember to close the annoying Google popup on the page\")\n",
    "        dfolder='./downloads'\n",
    "        geko_path='./geckodriver'\n",
    "        link='https://www.booking.com/index.es.html'\n",
    "        self.browser =start_up(dfolder=dfolder,link=link,geko_path=geko_path)\n",
    "        self.search_bar_xpath = '//div[@class=\"b9b84f4305\"]'\n",
    "        self.search_x_path= '/html/body/div[3]/div[2]/div/form/div[1]/div[4]/button/span'\n",
    "        self.search_x_path2 = '/html/body/div[4]/div/div[2]/div/div[1]/div/form/div[1]/div[4]/button'\n",
    "        self.date_button_css = 'button.ebbedaf8ac:nth-child(2) > span:nth-child(1)'\n",
    "        self.number_of_people_xpath = '/html/body/div[3]/div[2]/div/form/div[1]/div[3]/div/button'\n",
    "        self.search_button_xpath = '/html/body/div[3]/div[2]/div/form/div[1]/div[4]/button/span'\n",
    "        self.x_path_prev_date = '//button[@class=\"a83ed08757 c21c56c305 f38b6daa18 d691166b09 f671049264 deab83296e f4552b6561 dc72a8413c c9804790f7\"]'\n",
    "        self.x_path_next_date = '//button[@class=\"a83ed08757 c21c56c305 f38b6daa18 d691166b09 f671049264 deab83296e f4552b6561 dc72a8413c f073249358\"]'\n",
    "        self.x_path_month = '//h3[@class=\"e1eebb6a1e ee7ec6b631\"]'\n",
    "        x_path_cookies = '//button[@id=\"onetrust-accept-btn-handler\"]'\n",
    "        self.people_path = '/html/body/div[3]/div[2]/div/form/div[1]/div[3]/div/button'\n",
    "        self.pages = 1\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}\n",
    "        self.data = pd.DataFrame(columns=['Hotels', 'Ratings', 'Price', 'Link'])\n",
    "        self.place = ''\n",
    "        check_and_click(self.browser, x_path_cookies, 'xpath')\n",
    "    def input_place(self):\n",
    "        place = input('Where do you want to go?')\n",
    "        self.place = (place.lower()).capitalize()\n",
    "        self.browser.find_element(by='xpath', value='//div[@class=\"b9b84f4305\"]').click()\n",
    "        search = self.browser.find_element(by='xpath', value='//*[@id=\":re:\"]')\n",
    "        search.clear()\n",
    "        search.send_keys(place)\n",
    "        print(f'Place of stay: {(place.lower()).capitalize()}')\n",
    "        x_path_close = '/html/body/div[4]/div/div[2]/div/div[1]/div/form/div[1]/div[1]/div/div/div[1]/div/div/div[1]/span/svg'\n",
    "        check_and_click(self.browser, x_path_close, 'xpath')\n",
    "    def input_dates(self):\n",
    "        print(\"Just a second...\")\n",
    "        self.browser.find_element('css selector',self.date_button_css).click()\n",
    "        while element_exists(self.browser, self.x_path_prev_date):\n",
    "                self.browser.find_element('xpath', self.x_path_prev_date).click()\n",
    "                time.sleep(1)\n",
    "        start_date = (input(\"Input the start date of your programmed stay in the form (XX mes XXXX). Use Spanish month names. ¡Cuidado con la ortografía!\")).lower()\n",
    "        end_date = (input(\"Input the end date of your programmed stay in the form (XX mes XXXX). Use Spanish month names. ¡Cuidado con la ortografía!\")).lower()\n",
    "        month_and_year_start = start_date[3:]\n",
    "        month_and_year_end = end_date[3:]\n",
    "        month_and_year = self.browser.find_element('xpath', self.x_path_month).text\n",
    "        while month_and_year != month_and_year_start:\n",
    "                self.browser.find_element('xpath', self.x_path_next_date).click()\n",
    "                month_and_year = self.browser.find_element('xpath', self.x_path_month).text\n",
    "                time.sleep(1)\n",
    "        months_dict = {'enero': '01', 'febrero': '02', 'marzo': '03', 'abril': '04', 'mayo': '05', 'junio': '06', 'julio': '07', 'agosto': '08', 'septiembre': '09', 'octubre': '10', 'noviembre': '11', 'diciembre': '12'}\n",
    "        x_path_dates='//div[@id=\"calendar-searchboxdatepicker\"]//table[@class=\"eb03f3f27f\"]//tbody//td[@class=\"b80d5adb18\"]//span[@class=\"cf06f772fa\"]'\n",
    "        dates = self.browser.find_elements('xpath',x_path_dates)\n",
    "        from_day = start_date[:2]\n",
    "        to_day = end_date[:2]\n",
    "        month_start = month_and_year_start[:-5]\n",
    "        month_end = month_and_year_end[:-5]\n",
    "        year = month_and_year[-4:]\n",
    "        for date in dates:\n",
    "            if date.get_attribute(\"data-date\") == f\"{year}-{months_dict[month_start]}-{from_day}\":\n",
    "                date.click()\n",
    "                break\n",
    "        if month_start == month_end:\n",
    "            for date in dates:\n",
    "                if date.get_attribute(\"data-date\") == f\"{year}-{months_dict[month_end]}-{to_day}\":\n",
    "                    date.click()\n",
    "                    break\n",
    "        else:\n",
    "            while self.browser.find_element('xpath', self.x_path_month).text != month_and_year_end:\n",
    "                self.browser.find_element('xpath', self.x_path_next_date).click()\n",
    "            dates = self.browser.find_elements('xpath',x_path_dates)\n",
    "            for date in dates:\n",
    "                if date.get_attribute(\"data-date\") == f\"{year}-{months_dict[month_end]}-{to_day}\":\n",
    "                    date.click()\n",
    "                    break\n",
    "        self.browser.find_element('css selector',self.date_button_css).click()\n",
    "        print(f'Start date of the stay: {start_date}')\n",
    "        print(f'End date of the stay: {end_date}')\n",
    "    def input_people(self):\n",
    "        self.browser.find_element('xpath', self.people_path).click()\n",
    "        number_of_people = int(input('How many people in total are you looking an accomodation for?'))\n",
    "        css_minus = '/html/body/div[3]/div[2]/div/form/div[1]/div[3]/div/div/div/div/div[1]/div[2]/button[1]'\n",
    "        css_plus = '/html/body/div[3]/div[2]/div/form/div[1]/div[3]/div/div/div/div/div[1]/div[2]/button[2]'\n",
    "        if number_of_people == 1:\n",
    "            self.browser.find_element('xpath', css_minus).click()\n",
    "        elif number_of_people > 2:\n",
    "            i = 2\n",
    "            while i < number_of_people:\n",
    "                self.browser.find_element('xpath', css_plus).click()\n",
    "                i+=1\n",
    "                time.sleep(2)\n",
    "        self.browser.find_element('xpath', self.people_path).click()\n",
    "        print(f'Accomodations for: {number_of_people} people')\n",
    "    def search(self):\n",
    "        check_and_click(self.browser, self.search_x_path, 'xpath')\n",
    "        check_and_click(self.browser, self.search_x_path2, 'xpath')\n",
    "        place = self.place\n",
    "        print(f'Searching accomodations in {(place.lower()).capitalize()}...')\n",
    "    def get_pages(self, limit=None):\n",
    "        a = self.browser.find_elements('xpath', '//button[@class=\"a83ed08757 a2028338ea\"]')\n",
    "        if a:\n",
    "            total_pages = int(a[-1].text)\n",
    "            if limit is not None and total_pages > limit:\n",
    "                self.pages = limit\n",
    "            else:\n",
    "                self.pages = total_pages\n",
    "        else:\n",
    "            self.pages = 1\n",
    "    def scrape_info(self):\n",
    "        print(\"Scraping general info...\\n\")\n",
    "        change_page_xpath = '/html/body/div[4]/div/div[2]/div/div[2]/div[3]/div[2]/div[2]/div[4]/div[2]/nav/nav/div/div[3]/button/span/span'\n",
    "        css = 'div.b16a89683f:nth-child(3) > button:nth-child(1) > span:nth-child(1) > span:nth-child(1)'\n",
    "        first_page_xpath='/html/body/div[4]/div/div[2]/div/div[2]/div[3]/div[2]/div[2]/div[4]/div[2]/nav/nav/div/div[2]/ol/li[1]/button'\n",
    "        check_and_click(self.browser,first_page_xpath , type='xpath')\n",
    "        for i in range(self.pages):\n",
    "            print(f'Page: {i + 1}')\n",
    "            containers = self.browser.find_elements('xpath', '//div[@class=\"c066246e13\"]')\n",
    "            for hotel in containers:\n",
    "                hotel_name = hotel.find_element('xpath', './/div[@class=\"f6431b446c a15b38c233\"]').text\n",
    "                try:\n",
    "                    hotel_rating = hotel.find_element('xpath', './/div[@class=\"a3b8729ab1 d86cee9b25\"]').text\n",
    "                except:\n",
    "                    hotel_rating = np.nan\n",
    "                try:\n",
    "                    hotel_price = hotel.find_element('xpath', './/span[@class=\"f6431b446c fbfd7c1165 e84eb96b1f\"]').text\n",
    "                except:\n",
    "                    hotel_price = np.nan\n",
    "                try:\n",
    "                    url = hotel.find_element('xpath', './/a[@href]')\n",
    "                    hotel_url= url.get_attribute('href')\n",
    "                except:\n",
    "                    hotel_url = np.nan\n",
    "                new_row = {'Hotels': hotel_name, 'Ratings': hotel_rating, 'Price':hotel_price, 'Link': hotel_url}\n",
    "                self.data = pd.concat([self.data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            next = self.browser.find_element('css selector', css)\n",
    "            time.sleep(2)\n",
    "        print(\"\\nDone!\\n\")\n",
    "    def scrape_description(self,url):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status() \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        description_tag = soup.find('p', class_='a53cbfa6de b3efd73f69')\n",
    "\n",
    "        if description_tag:\n",
    "            return description_tag.get_text(strip=True)\n",
    "        else:\n",
    "            print(f\"Description tag not found on the page: {url}\")\n",
    "            return None\n",
    "    def get_descriptions(self):\n",
    "        print(\"Scraping descriptions...\")\n",
    "        num_threads = 16\n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            descriptions = []\n",
    "            for i, description in enumerate(executor.map(self.scrape_description, self.data['Link']), start=1):\n",
    "                descriptions.append(description)\n",
    "                if i % 50 == 0:\n",
    "                    print(f\"Scraped {i} links\")\n",
    "        self.data['Descriptions'] = descriptions\n",
    "        print(f\"Scraped {len(descriptions)} links in total\")\n",
    "        display(self.data)\n",
    "        print(\"\\nDone!\\n\")\n",
    "        return (self.data)\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e982e35",
   "metadata": {},
   "source": [
    "## Pipeline strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7eaa21",
   "metadata": {},
   "source": [
    "For scraping our data we have developed the \"Scrape\" class which contains all the necessary functions to inpute the data and scrape the website. The pipeline reported in the 2 code blocks below that instantiate the class and run the different methods, were developed as an interactive chatbot that gives the necessary information to inpute the data in the correct form and reports the step by step inputed data in a nice fashion to keep track of the scraping data inputed. This strategy makes the retrieval of the data from the website very pleasant and efficient. \n",
    "\n",
    "In the following section there is a step by step notebook to follow the scraping pipeline in an easier way and make the logic of the code more understandable with extensive comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ed575",
   "metadata": {},
   "source": [
    "### Pipeline (using the scraping class)\n",
    "\n",
    "#### 1. Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f868e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance1 = Scrape()\n",
    "instance1.input_place()\n",
    "instance1.input_dates()\n",
    "instance1.input_people()\n",
    "instance1.search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816d6ab",
   "metadata": {},
   "source": [
    "#### 2. Scraping general Information and Description text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e05322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'instance1.get_pages(limit = 2)\\ninstance1.scrape_info()\\ndata = instance1.get_descriptions()'"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance1.get_pages(limit = 2)\n",
    "instance1.scrape_info()\n",
    "data = instance1.get_descriptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88349c",
   "metadata": {},
   "source": [
    "### *3. Design a careful scraping pipeline that follows the advises seen in class and TAs. (5points) The basic points to bear in mind are:*\n",
    "\n",
    "+ *Organize the data you need, format and structure to store it beforehand. Try to foresee how you will need to read in the data to answer your questions. If you want, you can include some few lines explaining your pipeline strategy at the*\n",
    "+ *Codes should be as automated as possible. That is, you don't want to rely on human intervention to get your data.*\n",
    "\n",
    "+ *Use only the packages we have seen in the course. Although firefox is recommended, you can also use chrome as your scraping browser.*\n",
    "\n",
    "+ *Document your codes and make them robust and efficient.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c8b44",
   "metadata": {},
   "source": [
    "### Pipeline (Step by Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecdc516",
   "metadata": {},
   "source": [
    "### 1. Opening the Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "id": "b78b1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start-up the browser\n",
    "dfolder='./downloads'\n",
    "geko_path='./geckodriver'\n",
    "link='https://www.booking.com/index.es.html'\n",
    "\n",
    "\n",
    "browser=start_up(dfolder=dfolder,link=link,geko_path=geko_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316802b",
   "metadata": {},
   "source": [
    "### 2. Accepting Cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "id": "d07304fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: Element <button id=\"onetrust-accept-btn-handler\"> could not be scrolled into view\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:191:5\n",
      "ElementNotInteractableError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:351:5\n",
      "webdriverClickElement@chrome://remote/content/marionette/interaction.sys.mjs:166:11\n",
      "interaction.clickElement@chrome://remote/content/marionette/interaction.sys.mjs:135:11\n",
      "clickElement@chrome://remote/content/marionette/actors/MarionetteCommandsChild.sys.mjs:204:29\n",
      "receiveMessage@chrome://remote/content/marionette/actors/MarionetteCommandsChild.sys.mjs:84:31\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1037,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Click on \"Accept cookies\" button\n",
    "x_path_cookies = '//button[@id=\"onetrust-accept-btn-handler\"]'\n",
    "check_and_click(browser, x_path_cookies, 'xpath')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408cdedb",
   "metadata": {},
   "source": [
    "### 3. Search Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00244c2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'browser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m browser\u001b[38;5;241m.\u001b[39mfind_element(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpath\u001b[39m\u001b[38;5;124m'\u001b[39m,value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//div[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb9b84f4305\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mclick()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'browser' is not defined"
     ]
    }
   ],
   "source": [
    "# Click on the search bar\n",
    "browser.find_element(by='xpath',value='//div[@class=\"b9b84f4305\"]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f55ca",
   "metadata": {},
   "source": [
    "### 4. Input the place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "id": "b494432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the location to search for\n",
    "place = input('Where do you want to go?')\n",
    "search1 = browser.find_element(by='xpath',value='//*[@id=\":re:\"]')\n",
    "search1.send_keys(place)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab55e89",
   "metadata": {},
   "source": [
    "### 5. Input the Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "id": "8910628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and click the element to input the date\n",
    "css_date='button.ebbedaf8ac:nth-child(2) > span:nth-child(1)'\n",
    "browser.find_element('css selector',css_date).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "id": "29ef65fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junio 2024\n"
     ]
    }
   ],
   "source": [
    "x_path_prev_date = '//button[@class=\"a83ed08757 c21c56c305 f38b6daa18 d691166b09 f671049264 deab83296e f4552b6561 dc72a8413c c9804790f7\"]'\n",
    "while element_exists(browser, x_path_prev_date):\n",
    "        browser.find_element('xpath', x_path_prev_date).click()\n",
    "        time.sleep(1)\n",
    "x_path_month1 = '//h3[@class=\"e1eebb6a1e ee7ec6b631\"]'\n",
    "\n",
    "# Input the wanted date for the stay\n",
    "start_date = (input(\"Input the start date of your programmed stay in the form (XX mes XXXX). Use Spanish month names. ¡Cuidado con la ortografía!\")).lower()\n",
    "end_date = (input(\"Input the end date of your programmed stay in the form (XX mes XXXX). Use Spanish month names. ¡Cuidado con la ortografía!\")).lower()\n",
    "\n",
    "# Retrieve the current date on the screen and find the month of the start date inputed\n",
    "month_and_year_start = start_date[3:]\n",
    "month_and_year_end = end_date[3:]\n",
    "month_and_year = browser.find_element('xpath', x_path_month1).text\n",
    "x_path_next_date = '//button[@class=\"a83ed08757 c21c56c305 f38b6daa18 d691166b09 f671049264 deab83296e f4552b6561 dc72a8413c f073249358\"]'\n",
    "while month_and_year != month_and_year_start:\n",
    "        browser.find_element('xpath', x_path_next_date).click()\n",
    "        month_and_year = browser.find_element('xpath', x_path_month1).text\n",
    "        time.sleep(1)\n",
    "print(month_and_year)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7203895c",
   "metadata": {},
   "source": [
    "### 6. Select the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "id": "5b9f8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_dict = {'enero': '01', 'febrero': '02', 'marzo': '03', 'abril': '04', 'mayo': '05', 'junio': '06', 'julio': '07', 'agosto': '08', 'septiembre': '09', 'octubre': '10', 'noviembre': '11', 'diciembre': '12'}\n",
    "x_path_dates='//div[@id=\"calendar-searchboxdatepicker\"]//table[@class=\"eb03f3f27f\"]//tbody//td[@class=\"b80d5adb18\"]//span[@class=\"cf06f772fa\"]'\n",
    "dates = browser.find_elements('xpath',x_path_dates)\n",
    "from_day = start_date[:2]\n",
    "to_day = end_date[:2]\n",
    "month_start = month_and_year_start[:-5]\n",
    "month_end = month_and_year_end[:-5]\n",
    "year = month_and_year[-4:]\n",
    "\n",
    "# Select the dates\n",
    "for date in dates:\n",
    "    if date.get_attribute(\"data-date\") == f\"{year}-{months_dict[month_start]}-{from_day}\":\n",
    "        date.click()\n",
    "        break\n",
    "if month_start == month_end:\n",
    "    for date in dates:\n",
    "        if date.get_attribute(\"data-date\") == f\"{year}-{months_dict[month_end]}-{to_day}\":\n",
    "            date.click()\n",
    "            break\n",
    "else:\n",
    "    while browser.find_element('xpath', x_path_month1).text != month_and_year_end:\n",
    "        browser.find_element('xpath', x_path_next_date).click()\n",
    "    dates = browser.find_elements('xpath',x_path_dates)\n",
    "    for date in dates:\n",
    "        if date.get_attribute(\"data-date\") == f\"{year}-{months_dict[month_end]}-{to_day}\":\n",
    "            date.click()\n",
    "            break\n",
    "browser.find_element('css selector',css_date).click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb44508",
   "metadata": {},
   "source": [
    "### 7. Input the number of people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "id": "8af2effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_path = '/html/body/div[3]/div[2]/div/form/div[1]/div[3]/div/button'\n",
    "\n",
    "browser.find_element('xpath', x_path).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "id": "612cf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of people\n",
    "number_of_people = int(input('How many people in total are you looking an accomodation for?'))\n",
    "\n",
    "css_minus = '/html/body/div[3]/div[2]/div/form/div[1]/div[3]/div/div/div/div/div[1]/div[2]/button[1]'\n",
    "css_plus = '/html/body/div[3]/div[2]/div/form/div[1]/div[3]/div/div/div/div/div[1]/div[2]/button[2]'\n",
    "if number_of_people == 1:\n",
    "    browser.find_element('xpath', css_minus).click()\n",
    "elif number_of_people > 2:\n",
    "    i = 2\n",
    "    while i < number_of_people:\n",
    "        browser.find_element('xpath', css_plus).click()\n",
    "        i+=1\n",
    "        time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37316444",
   "metadata": {},
   "source": [
    "### 8. Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on the search button\n",
    "search_xpath='/html/body/div[3]/div[2]/div/form/div[1]/div[4]/button/span'\n",
    "\n",
    "check_obscures(browser,search_xpath , type='xpath')\n",
    "check_and_click(browser,search_xpath , type='xpath')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d0e1b",
   "metadata": {},
   "source": [
    "### 9. Extracting Number of Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "id": "a586e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "def get_number_pages(browser):\n",
    "    '''\n",
    "    Get the number of pages. \n",
    "    '''\n",
    "    a = browser.find_elements('xpath',\n",
    "        '//button[@class=\"a83ed08757 a2028338ea\"]')\n",
    "    if a:\n",
    "        return(int(a[-1].text))\n",
    "    else:\n",
    "        return (1)\n",
    "\n",
    "pages = get_number_pages(browser)\n",
    "\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db15a4",
   "metadata": {},
   "source": [
    "### *4. Scrape date, room price, hotel name and hotel description. (5 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c23443",
   "metadata": {},
   "source": [
    "### 10. Scraping Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "id": "7a45b2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1\n",
      "Page: 2\n",
      "Page: 3\n",
      "Page: 4\n",
      "Page: 5\n",
      "Page: 6\n",
      "Page: 7\n",
      "Page: 8\n",
      "Page: 9\n",
      "Page: 10\n",
      "Page: 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#Y101sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m containers \u001b[39m=\u001b[39m browser\u001b[39m.\u001b[39mfind_elements(\u001b[39m'\u001b[39m\u001b[39mxpath\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m//div[@class=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mc066246e13\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#Y101sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m hotel \u001b[39min\u001b[39;00m containers:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#Y101sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     hotel_name \u001b[39m=\u001b[39m hotel\u001b[39m.\u001b[39mfind_element(\u001b[39m'\u001b[39m\u001b[39mxpath\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.//div[@class=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf6431b446c a15b38c233\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtext\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#Y101sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#Y101sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         hotel_rating \u001b[39m=\u001b[39m hotel\u001b[39m.\u001b[39mfind_element(\u001b[39m'\u001b[39m\u001b[39mxpath\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.//div[@class=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ma3b8729ab1 d86cee9b25\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:90\u001b[0m, in \u001b[0;36mWebElement.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     89\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"The text of the element.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute(Command\u001b[39m.\u001b[39mGET_ELEMENT_TEXT)[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    393\u001b[0m     params \u001b[39m=\u001b[39m {}\n\u001b[1;32m    394\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id\n\u001b[0;32m--> 395\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent\u001b[39m.\u001b[39mexecute(command, params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:346\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    344\u001b[0m         params[\u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m--> 346\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    348\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:300\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    298\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[1;32m    299\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 300\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(command_info[\u001b[39m0\u001b[39m], url, body\u001b[39m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:321\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    318\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 321\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conn\u001b[39m.\u001b[39mrequest(method, url, body\u001b[39m=\u001b[39mbody, headers\u001b[39m=\u001b[39mheaders)\n\u001b[1;32m    322\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:74\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m urlopen_kw[\u001b[39m\"\u001b[39m\u001b[39mrequest_url\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m url\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_url_methods:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:96\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m fields:\n\u001b[1;32m     94\u001b[0m     url \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m urlencode(fields)\n\u001b[0;32m---> 96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, u\u001b[39m.\u001b[39mrequest_uri, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m # Make the request on the httplib connection object.\n\u001b[1;32m    703\u001b[0m httplib_response = self._make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m     chunked=chunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m # If we're going to release the connection in ``finally:``, then\n\u001b[0;32m--> 714\u001b[0m # the response doesn't need to know about the connection. Otherwise\n\u001b[1;32m    715\u001b[0m # it will also try to release it and we'll have a double-release\n\u001b[1;32m    716\u001b[0m # mess.\n\u001b[1;32m    717\u001b[0m response_conn = conn if not release_conn else None\n\u001b[1;32m    719\u001b[0m # Pass method to Response for length checking\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39m# AppEngine doesn't have a version attr.\u001b[39;00m\n\u001b[1;32m    455\u001b[0m http_version \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39m_http_vsn_str\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mHTTP/?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    456\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    457\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m://\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m    458\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheme,\n\u001b[1;32m    459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost,\n\u001b[1;32m    460\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport,\n\u001b[1;32m    461\u001b[0m     method,\n\u001b[1;32m    462\u001b[0m     url,\n\u001b[1;32m    463\u001b[0m     http_version,\n\u001b[1;32m    464\u001b[0m     httplib_response\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    465\u001b[0m     httplib_response\u001b[39m.\u001b[39mlength,\n\u001b[0;32m--> 466\u001b[0m )\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39m# AppEngine doesn't have a version attr.\u001b[39;00m\n\u001b[1;32m    455\u001b[0m http_version \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39m_http_vsn_str\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mHTTP/?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    456\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    457\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m://\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m    458\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheme,\n\u001b[1;32m    459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost,\n\u001b[1;32m    460\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport,\n\u001b[0;32m--> 461\u001b[0m     method,\n\u001b[1;32m    462\u001b[0m     url,\n\u001b[1;32m    463\u001b[0m     http_version,\n\u001b[1;32m    464\u001b[0m     httplib_response\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    465\u001b[0m     httplib_response\u001b[39m.\u001b[39mlength,\n\u001b[1;32m    466\u001b[0m )\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finding the button to change the page in Booking.com\n",
    "change_page_xpath = '/html/body/div[4]/div/div[2]/div/div[2]/div[3]/div[2]/div[2]/div[4]/div[2]/nav/nav/div/div[3]/button/span/span'\n",
    "css = 'div.b16a89683f:nth-child(3) > button:nth-child(1) > span:nth-child(1) > span:nth-child(1)'\n",
    "# Creating DataFrame\n",
    "data = pd.DataFrame(columns=['Hotels', 'Ratings', 'Price', 'Link'])\n",
    "# Make sure to be on the first page when starting to scrape the data\n",
    "first_page_xpath='/html/body/div[4]/div/div[2]/div/div[2]/div[3]/div[2]/div[2]/div[4]/div[2]/nav/nav/div/div[2]/ol/li[1]/button'\n",
    "check_and_click(browser,first_page_xpath , type='xpath')\n",
    "# loop to scrape the data and populate the DataFrame\n",
    "for i in range(pages):\n",
    "    print(f'Page: {i + 1}')\n",
    "    # Dividing the page in the Container Objects, one for every hotel and extracting the wanted data from each\n",
    "    containers = browser.find_elements('xpath', '//div[@class=\"c066246e13\"]')\n",
    "    for hotel in containers:\n",
    "        hotel_name = hotel.find_element('xpath', './/div[@class=\"f6431b446c a15b38c233\"]').text\n",
    "        try:\n",
    "            hotel_rating = hotel.find_element('xpath', './/div[@class=\"a3b8729ab1 d86cee9b25\"]').text\n",
    "        except:\n",
    "            hotel_rating = np.nan\n",
    "        try:\n",
    "            hotel_price = hotel.find_element('xpath', './/span[@class=\"f6431b446c fbfd7c1165 e84eb96b1f\"]').text\n",
    "        except:\n",
    "            hotel_price = np.nan\n",
    "        try:\n",
    "            url = hotel.find_element('xpath', './/a[@href]')\n",
    "            hotel_url= url.get_attribute('href')\n",
    "        except:\n",
    "            hotel_url = np.nan\n",
    "        new_row = {'Hotels': hotel_name, 'Ratings': hotel_rating, 'Price':hotel_price, 'Link': hotel_url}\n",
    "        data = pd.concat([data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    # Change page with CSS Selector\n",
    "    next = browser.find_element('css selector', css)\n",
    "    time.sleep(2)\n",
    "print(\"\\nDone!\\n\")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e9f2c",
   "metadata": {},
   "source": [
    "### 11. Scraping Descriptions using BeutifulSoup with parrallelized operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6dc5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 50 links\n",
      "Scraped 100 links\n",
      "Scraped 150 links\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb Cell 41\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#X51sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mnum_threads) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#X51sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m# Use executor.map to apply the scrape_description function to each URL in parallel\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#X51sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     descriptions \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#X51sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, description \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(executor\u001b[39m.\u001b[39mmap(scrape_description, data[\u001b[39m'\u001b[39m\u001b[39mLink\u001b[39m\u001b[39m'\u001b[39m]), start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#X51sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         descriptions\u001b[39m.\u001b[39mappend(description)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathieu26/Desktop/DSDM-BSE/Term_2/Text_Mining_and_Natural_Lan_Processing/Assignment1/Webscrapping_Booking.Com-1/scraping_nb.ipynb#X51sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m# Print every 50 link to check the progess of the scraping\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}\n",
    "\n",
    "# Function to scrape the descriptions using Beautiful Soup\n",
    "def scrape_description(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status() \n",
    "        # time.sleep(0.5)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    description_tag = soup.find('p', class_='a53cbfa6de b3efd73f69')\n",
    "\n",
    "    if description_tag:\n",
    "        return description_tag.get_text(strip=True)\n",
    "    else:\n",
    "        print(f\"Description tag not found on the page: {url}\")\n",
    "        return None\n",
    "\n",
    "# Set the number of concurrent threads (adjust this based on the processing power of your computer\n",
    "num_threads = 16\n",
    "\n",
    "# Create a ThreadPoolExecutor to run operations in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    # Use executor.map to apply the scrape_description function to each URL in parallel\n",
    "    descriptions = []\n",
    "    for i, description in enumerate(executor.map(scrape_description, data['Link']), start=1):\n",
    "        descriptions.append(description)\n",
    "        # Print every 50 link to check the progess of the scraping\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Scraped {i} links\")\n",
    "\n",
    "# Assign the descriptions to the 'Descriptions' column in the DataFrame\n",
    "data['Descriptions'] = descriptions\n",
    "\n",
    "# Print count after all threads have completed\n",
    "print(f\"Scraped {len(descriptions)} links\")\n",
    "print(\"\\nDone!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53636425",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a2b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotels</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "      <th>Link</th>\n",
       "      <th>Descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Room Mate Gerard</td>\n",
       "      <td>8,8</td>\n",
       "      <td>€ 546</td>\n",
       "      <td>https://www.booking.com/hotel/es/room-mate-ger...</td>\n",
       "      <td>El Room Mate Gerard en Barcelona ofrece alojam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sonder Los Arcos</td>\n",
       "      <td>8,4</td>\n",
       "      <td>€ 653</td>\n",
       "      <td>https://www.booking.com/hotel/es/sonder-los-ar...</td>\n",
       "      <td>Sonder Los Arcos dispone de alojamiento con wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Catalonia Sagrada Familia</td>\n",
       "      <td>8,2</td>\n",
       "      <td>€ 337</td>\n",
       "      <td>https://www.booking.com/hotel/es/cataloniaarag...</td>\n",
       "      <td>El Catalonia Sagrada Familia se halla a 15 min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Catalonia Diagonal Centro</td>\n",
       "      <td>8,4</td>\n",
       "      <td>€ 400</td>\n",
       "      <td>https://www.booking.com/hotel/es/catalonia-dia...</td>\n",
       "      <td>El Catalonia Diagonal Centro se encuentra en p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hostal Santa Ana</td>\n",
       "      <td>7,0</td>\n",
       "      <td>€ 174</td>\n",
       "      <td>https://www.booking.com/hotel/es/hostal-cortes...</td>\n",
       "      <td>El Hostal Santa Ana se encuentra en pleno cent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>Hostal Boqueria</td>\n",
       "      <td>8,4</td>\n",
       "      <td>€ 422</td>\n",
       "      <td>https://www.booking.com/hotel/es/hostal-boquer...</td>\n",
       "      <td>Este establecimiento se encuentra en las Rambl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>Catalonia Passeig de Gràcia 4* Sup</td>\n",
       "      <td>9,0</td>\n",
       "      <td>€ 570</td>\n",
       "      <td>https://www.booking.com/hotel/es/catalonia-pas...</td>\n",
       "      <td>El Catalonia Passeig de Gràcia 4* Sup se encue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>Hostal Benidorm</td>\n",
       "      <td>8,2</td>\n",
       "      <td>€ 332</td>\n",
       "      <td>https://www.booking.com/hotel/es/hostal-benido...</td>\n",
       "      <td>El Hostal Benidorm está situado en el famoso p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>Hostal Conde Güell</td>\n",
       "      <td>8,6</td>\n",
       "      <td>€ 179</td>\n",
       "      <td>https://www.booking.com/hotel/es/hostal-conde-...</td>\n",
       "      <td>El Hostal Conde Güell está ubicado en Barcelon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>Ilunion Almirante</td>\n",
       "      <td>7,4</td>\n",
       "      <td>€ 422</td>\n",
       "      <td>https://www.booking.com/hotel/es/almirante.es....</td>\n",
       "      <td>Este hotel elegante se encuentra entre el Barr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>858 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Hotels Ratings  Price  \\\n",
       "0                      Room Mate Gerard     8,8  € 546   \n",
       "1                      Sonder Los Arcos     8,4  € 653   \n",
       "2             Catalonia Sagrada Familia     8,2  € 337   \n",
       "3             Catalonia Diagonal Centro     8,4  € 400   \n",
       "4                      Hostal Santa Ana     7,0  € 174   \n",
       "..                                  ...     ...    ...   \n",
       "853                     Hostal Boqueria     8,4  € 422   \n",
       "854  Catalonia Passeig de Gràcia 4* Sup     9,0  € 570   \n",
       "855                     Hostal Benidorm     8,2  € 332   \n",
       "856                  Hostal Conde Güell     8,6  € 179   \n",
       "857                   Ilunion Almirante     7,4  € 422   \n",
       "\n",
       "                                                  Link  \\\n",
       "0    https://www.booking.com/hotel/es/room-mate-ger...   \n",
       "1    https://www.booking.com/hotel/es/sonder-los-ar...   \n",
       "2    https://www.booking.com/hotel/es/cataloniaarag...   \n",
       "3    https://www.booking.com/hotel/es/catalonia-dia...   \n",
       "4    https://www.booking.com/hotel/es/hostal-cortes...   \n",
       "..                                                 ...   \n",
       "853  https://www.booking.com/hotel/es/hostal-boquer...   \n",
       "854  https://www.booking.com/hotel/es/catalonia-pas...   \n",
       "855  https://www.booking.com/hotel/es/hostal-benido...   \n",
       "856  https://www.booking.com/hotel/es/hostal-conde-...   \n",
       "857  https://www.booking.com/hotel/es/almirante.es....   \n",
       "\n",
       "                                          Descriptions  \n",
       "0    El Room Mate Gerard en Barcelona ofrece alojam...  \n",
       "1    Sonder Los Arcos dispone de alojamiento con wi...  \n",
       "2    El Catalonia Sagrada Familia se halla a 15 min...  \n",
       "3    El Catalonia Diagonal Centro se encuentra en p...  \n",
       "4    El Hostal Santa Ana se encuentra en pleno cent...  \n",
       "..                                                 ...  \n",
       "853  Este establecimiento se encuentra en las Rambl...  \n",
       "854  El Catalonia Passeig de Gràcia 4* Sup se encue...  \n",
       "855  El Hostal Benidorm está situado en el famoso p...  \n",
       "856  El Hostal Conde Güell está ubicado en Barcelon...  \n",
       "857  Este hotel elegante se encuentra entre el Barr...  \n",
       "\n",
       "[858 rows x 5 columns]"
      ]
     },
     "execution_count": 970,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf1b9a2",
   "metadata": {},
   "source": [
    "### 12. Create CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file with the scraped data (Adjust the path)\n",
    "data.to_csv('Data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('pyannote')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e5bc56a5e59828724b9606669ef2e877f786cc96978258679ba8b9f5956bc86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
